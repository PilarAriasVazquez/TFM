{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd22246",
   "metadata": {},
   "source": [
    "# Generar voz con IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8193d",
   "metadata": {},
   "source": [
    "## INGL√âS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc7991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script para grabar voz \n",
    "import tortoise.api as api\n",
    "import tortoise.utils as utils\n",
    "import os\n",
    "from scipy.io.wavfile import write\n",
    "# Ruta a los clips de voz\n",
    "clips_paths = [\n",
    "    \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_1.wav\",\n",
    "    \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_2.wav\",\n",
    "    \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_3.wav\",\n",
    "    \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_4.wav\"\n",
    "]\n",
    "\n",
    "reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]\n",
    "tts = api.TextToSpeech()\n",
    "pcm_audio = tts.tts_with_preset(\"Hola, soy Pilar Arias y tengo 23 a√±os. Me gusta el caf√© y el agua\", voice_samples=reference_clips, preset='fast')\n",
    "# Guardar el audio generado\n",
    "output_path = \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/output.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e09665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = \"voz_clonada.wav\"\n",
    "write(output_path, 24000, pcm_audio.squeeze().cpu().numpy())\n",
    "print(f\"‚úÖ Audio guardado como {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f429c",
   "metadata": {},
   "source": [
    "## ESPA√ëOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6217cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import random\n",
    "from TTS.api import TTS\n",
    "from torchaudio.transforms import MelSpectrogram, MFCC\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.serialization\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "\n",
    "# A√±adir XttsConfig como clase segura\n",
    "torch.serialization.add_safe_globals({\"TTS.tts.configs.xtts_config.XttsConfig\": XttsConfig})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99fcf64",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/68835622-2248-8001-9e16-a1eb0cebf76f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b78446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 1. Generar audios falsos con Coqui XTTS\n",
    "# ============================\n",
    "\n",
    "\n",
    "audio_dir = Path(\"Audios_48samplerate\")\n",
    "speaker_wavs = list(audio_dir.rglob(\"*.wav\"))\n",
    "\n",
    "# todos los audios dentro de Audios_48samplerate\n",
    "\n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n",
    "\n",
    "texts_fake = [\n",
    "    \"Hola, soy del banco, Necesitamos verificar una transacci√≥n sospechosa.\",\n",
    "    \"Detectamos un movimiento extra√±o en tu cuenta, Por favor, confirma tu identidad.\",\n",
    "    \"Se ha intentado acceder a tu cuenta desde otro dispositivo, Activa tu token.\",\n",
    "    \"Necesitamos tu n√∫mero de tarjeta para validar el pago.\",\n",
    "    \"Buenas tardes, hemos detectado un fallo de seguridad en tu sistema.\",\n",
    "    \"Buenas, le llamamos del banco por un posible fraude en su cuenta.\",\n",
    "    \"Para evitar el bloqueo de su tarjeta, necesitamos el c√≥digo CVV.\",\n",
    "    \"Su cuenta ha sido comprometida, Necesitamos verificar sus credenciales.\",\n",
    "    \"Hola, soy del servicio de verificaci√≥n, Necesitamos validar sus datos personales.\",\n",
    "    \"Hemos detectado una compra sospechosa en su cuenta de Amazon.\",\n",
    "    \"Debe validar su identidad para evitar el cierre de su cuenta.\",\n",
    "    \"Llamamos del servicio t√©cnico, Su ordenador est√° infectado.\",\n",
    "    \"Necesitamos acceso remoto para reparar un fallo en su sistema.\",\n",
    "    \"Tiene un reembolso pendiente, Facilite su n√∫mero de cuenta.\",\n",
    "    \"Para continuar usando su app, confirme los datos por SMS.\",\n",
    "    \"La polic√≠a est√° investigando su cuenta, Necesitamos colaboraci√≥n urgente.\",\n",
    "    \"Se ha bloqueado su acceso online por motivos de seguridad.\",\n",
    "    \"Su dispositivo est√° en riesgo, Debe instalar una herramienta oficial.\",\n",
    "    \"Para evitar multas, actualice su informaci√≥n fiscal ahora.\",\n",
    "    \"Ha ganado un premio, Env√≠e sus datos para recibirlo.\",\n",
    "    \"Le llamamos de la Seguridad Social, Faltan datos en su expediente.\",\n",
    "    \"Confirmamos que su tarjeta ha sido clonada, Necesitamos los datos actuales.\",\n",
    "    \"Hola, ha recibido una sanci√≥n, Ev√≠tela accediendo al siguiente enlace.\",\n",
    "    \"Detectamos actividad irregular, Necesitamos comprobar el √∫ltimo movimiento.\",\n",
    "    \"Hay un cargo sospechoso, Para anularlo, indique su n√∫mero de cuenta.\"\n",
    "] + [\n",
    "    \"Hola, ¬øvas a venir hoy a clase?\",\n",
    "    \"Estoy terminando el informe. Te lo paso esta tarde.\",\n",
    "    \"Acu√©rdate de comprar leche.\",\n",
    "    \"Voy a salir a correr un rato.\",\n",
    "    \"Nos vemos en el cine a las 7.\",\n",
    "    \"¬øTe apetece tomar algo esta tarde?\",\n",
    "    \"Voy a hacer la compra, ¬ønecesitas algo?\",\n",
    "    \"He terminado el informe, te lo env√≠o en un rato.\",\n",
    "    \"¬øTe viene bien si pasamos por tu casa a las seis?\",\n",
    "    \"No te olvides de llevar el paraguas.\",\n",
    "    \"Ma√±ana tengo reuni√≥n temprano, no podr√© ir al gimnasio.\",\n",
    "    \"Estamos organizando una cena el viernes. ¬øTe apuntas?\",\n",
    "    \"¬øPuedes recoger el paquete de la oficina de correos?\",\n",
    "    \"He dejado las llaves en la mesa del sal√≥n.\",\n",
    "    \"El examen de ma√±ana empieza a las 9 en punto.\",\n",
    "    \"Voy a preparar algo de cenar, ¬øquieres que te guarde?\",\n",
    "    \"No me esperes despierto, llegar√© tarde.\",\n",
    "    \"¬øQuieres que te mande las fotos del viaje?\",\n",
    "    \"Recuerda que tenemos cita con el m√©dico el jueves.\",\n",
    "    \"Te escribo cuando llegue al trabajo.\",\n",
    "    \"¬øVienes a la reuni√≥n o te conectas online?\",\n",
    "    \"Acordamos vernos en la estaci√≥n a las cinco.\",\n",
    "    \"Lleva abrigo, est√° haciendo mucho fr√≠o.\",\n",
    "    \"Salgo en diez minutos, te aviso al llegar.\",\n",
    "    \"¬øTe llamo cuando est√© libre o prefieres que te escriba?\"\n",
    "]\n",
    "\n",
    "output_dir = Path(\"fake_audios\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for i, text in enumerate(texts_fake):\n",
    "    wav = tts.tts(text, speaker_wav=speaker_wavs, language=\"es\")\n",
    "    torchaudio.save(str(output_dir / f\"fake_{i:03}.wav\"), torch.tensor([wav]), 24000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e474af",
   "metadata": {},
   "source": [
    "# CLASIFICADOR VOCES MFCC + PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ffb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 2. Extraer MFCCs de audios reales y falsos\n",
    "# =============================================\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "def extract_mfcc(file_path, n_mfcc=13):\n",
    "    y, sr = librosa.load(file_path, sr=24000)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return mfcc.mean(axis=1)\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Audios reales\n",
    "real_dir = \"Audios_48samplerate/Normal\"\n",
    "fake_dir = \"fake_audios\"\n",
    "for file in os.listdir(real_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        features = extract_mfcc(os.path.join(real_dir, file))\n",
    "        data.append(features)\n",
    "        labels.append(0)  # 0 = real\n",
    "\n",
    "# Audios falsos\n",
    "for file in os.listdir(fake_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        features = extract_mfcc(os.path.join(\"fake_audios\", file))\n",
    "        data.append(features)\n",
    "        labels.append(1)  # 1 = fake\n",
    "\n",
    "X = np.array(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "# =============================================\n",
    "# 3. Clasificador con PyTorch\n",
    "# =============================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Convertir a tensores con tipos correctos\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Dividir en train, validaci√≥n y test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "# 0.6 train / 0.2 val / 0.2 test\n",
    "\n",
    "# Datasets y dataloaders\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=8)\n",
    "test_dl = DataLoader(test_ds, batch_size=8)\n",
    "\n",
    "# Modelo\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(13, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Classifier().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(40):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    # Validaci√≥n\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            acc += (pred.argmax(1) == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    print(f\"üìä Epoch {epoch+1:02d} - Validaci√≥n: Accuracy = {acc/total:.2%}\")\n",
    "\n",
    "# Evaluaci√≥n en test\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        pred = model(xb).argmax(1).cpu()\n",
    "        all_preds.extend(pred.numpy())\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "print(\"\\nüß™ Resultados en Test:\")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    labels=[0, 1],\n",
    "    target_names=[\"Real\", \"Falsa\"],\n",
    "    zero_division=0  # evita errores si una clase no est√° presente\n",
    "))\n",
    "\n",
    "print(\"Matriz de confusi√≥n:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ec529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Distribuci√≥n original:\", Counter(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3184b4",
   "metadata": {},
   "source": [
    "# CLASIFICADOR VOCES PRE-ENTRENADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cae86",
   "metadata": {},
   "source": [
    "## Detecta todo como fake :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e9f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "# Carga del modelo y extractor\n",
    "\n",
    "##################################33 TODO FAKE\n",
    "# model = AutoModelForAudioClassification.from_pretrained(\"mo-thecreator/Deepfake-audio-detection\")\n",
    "# feature_extractor = AutoFeatureExtractor.from_pretrained(\"mo-thecreator/Deepfake-audio-detection\")\n",
    "\n",
    "\n",
    "##################################33 TODO FAKE\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"langulor/deepfake-voice-spanish\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"langulor/deepfake-voice-spanish\")\n",
    "\n",
    "\n",
    "\n",
    "# Cargar un archivo de audio (.wav, 16kHz)\n",
    "waveform, sample_rate = torchaudio.load(\"Audios_48samplerate/Normal/sample_1_20250721_173507.wav\")\n",
    "waveform = waveform.mean(dim=0)  # Convertir a mono si est√° en est√©reo\n",
    "\n",
    "# Resamplear si es necesario\n",
    "if sample_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "    sample_rate = 16000\n",
    "\n",
    "# Extraer caracter√≠sticas\n",
    "inputs = feature_extractor(waveform.numpy(), sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "\n",
    "classes = model.config.id2label\n",
    "print(f\"Clases: {classes}\")\n",
    "# Clasificaci√≥n\n",
    "with torch.no_grad(): #evaluar el modelo sin calcular gradientes\n",
    "    model.eval()  # Poner el modelo en modo evaluaci√≥n\n",
    "    logits = model(**inputs).logits\n",
    "    probs = softmax(logits, dim=0)\n",
    "    predicted_class = torch.argmax(logits).item()\n",
    "\n",
    "# Interpretar resultado\n",
    "if predicted_class == 0:\n",
    "    print(\"üîä El audio parece **real**.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è El audio parece **deepfake**.\")\n",
    "\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos audio output.wav\n",
    "waveform, sample_rate = torchaudio.load(\"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/output.wav\")\n",
    "waveform = waveform.mean(dim=0)  # Convertir a mono si est√° en est√©reo\n",
    "print(sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4e40c",
   "metadata": {},
   "source": [
    "# AUDIO TO TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingsound import SpeechRecognitionModel\n",
    "import os\n",
    "from pathlib import Path\n",
    "model = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-spanish\")\n",
    "# audio_paths = [r\"fake_audios\\fake_000.wav\", r\"Audios_48samplerate\\Normal\\sample_4_20250721_173627.wav\"]\n",
    "\n",
    "# all audios in Audios_48samplerate/Normal\n",
    "audio_paths = [str(p) for p in Path(\"Audios_48samplerate/Normal\").rglob(\"*.wav\")]\n",
    "\n",
    "transcriptions = model.transcribe(audio_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcbfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get transcriptions\n",
    "for i, transcription in enumerate(transcriptions):\n",
    "    print(f\"Transcripci√≥n {i+1}:    {transcription['transcription']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a953f",
   "metadata": {},
   "source": [
    "## La cosa ahora ser√≠a integrar esto en un audio que se est√° generando en vivo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee5ff9f",
   "metadata": {},
   "source": [
    "La siguiente celda de c√≥digo cuenta con: \n",
    "\n",
    "- CHUNK_DURATION = 0.8 // latencia muy baja (~1‚ÄØs) -> nos da el texto asap con el menor delay\n",
    "\n",
    "- blocksize peque√±o (blocksize=512)  el callback se activa r√°pido.\n",
    "\n",
    "- Umbral de energ√≠a RMS (threshold_db = -40) para detectar si hay voz antes de transcribir // as√≠ ahorramos coste computacional\n",
    "\n",
    "- Todo en memoria, sin escribir WAVs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bec13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from transformers import pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# üîß CONFIGURACI√ìN DEL AUDIO\n",
    "# -----------------------------\n",
    "DEVICE_NAME = \"CABLE Output (VB-Audio Virtual Cable)\"\n",
    "FS = 16000              # Wav2Vec2 requiere 16kHz\n",
    "CHUNK_DURATION = 0.8    # Segundos por fragmento (baja latencia)\n",
    "BLOCKSIZE = int(FS * CHUNK_DURATION)\n",
    "ENERGY_THRESHOLD_DB = -40  # Umbral en decibelios para detecci√≥n de voz\n",
    "\n",
    "# Buscar √≠ndice del dispositivo\n",
    "def buscar_dispositivo(nombre_dispositivo):\n",
    "    for i, dev in enumerate(sd.query_devices()):\n",
    "        if nombre_dispositivo.lower() in dev['name'].lower():\n",
    "            return i\n",
    "    raise ValueError(f\"Dispositivo '{nombre_dispositivo}' no encontrado.\")\n",
    "\n",
    "try:\n",
    "    dispositivo_idx = buscar_dispositivo(DEVICE_NAME)\n",
    "    print(f\"üéß Usando dispositivo: {DEVICE_NAME} (√≠ndice {dispositivo_idx})\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# -----------------------------\n",
    "# üß† CARGAR MODELO wav2vec2\n",
    "# -----------------------------\n",
    "print(\"üîÑ Cargando modelo...\")\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"jonatasgrosman/wav2vec2-large-xlsr-53-spanish\",\n",
    "    device=0 if sd.query_devices(kind='input')['hostapi'] == 1 else -1  # GPU si disponible\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# üéôÔ∏è CAPTURA Y TRANSCRIPCI√ìN\n",
    "# -----------------------------\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    if status:\n",
    "        print(f\"‚ö†Ô∏è Estado del audio: {status}\")\n",
    "    audio_queue.put(indata.copy())\n",
    "\n",
    "def energia_rms_db(audio_array):\n",
    "    # RMS -> log en dB\n",
    "    rms = np.sqrt(np.mean(audio_array**2))\n",
    "    if rms == 0:\n",
    "        return -100\n",
    "    return 20 * np.log10(rms)\n",
    "\n",
    "def transcribir_en_vivo():\n",
    "    print(\"üß† Transcribiendo en tiempo real con buffer de texto...\\n\")\n",
    "    buffer = np.empty((0, 1), dtype=np.float32)\n",
    "    texto_acumulado = \"\"\n",
    "    tiempo_ultimo_envio = time.time()\n",
    "\n",
    "    while True:\n",
    "        data = audio_queue.get()\n",
    "        buffer = np.concatenate((buffer, data), axis=0)\n",
    "\n",
    "        if len(buffer) >= BLOCKSIZE:\n",
    "            chunk = buffer[:BLOCKSIZE]\n",
    "            buffer = buffer[BLOCKSIZE:]\n",
    "\n",
    "            audio_array = chunk.squeeze()\n",
    "            energia_db = energia_rms_db(audio_array)\n",
    "\n",
    "            if energia_db < ENERGY_THRESHOLD_DB:\n",
    "                print(f\"üîá Fragmento descartado (RMS = {energia_db:.1f} dB)\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = asr_pipeline(audio_array)\n",
    "                texto = result['text'].strip()\n",
    "\n",
    "                # Filtro: solo a√±adir si tiene 3 o m√°s palabras\n",
    "                if len(texto.split()) >= 3:\n",
    "                    texto_acumulado += \" \" + texto\n",
    "                    print(\"üó£Ô∏è\", texto)\n",
    "\n",
    "                # Enviar al clasificador si ha pasado suficiente tiempo o el buffer es largo\n",
    "                if len(texto_acumulado.split()) >= 20 or (time.time() - tiempo_ultimo_envio) > 10:\n",
    "                    if texto_acumulado:\n",
    "                        print(\"\\nüîé Texto para clasificaci√≥n:\", texto_acumulado.strip())\n",
    "\n",
    "                        texto_acumulado = \"\"\n",
    "                        tiempo_ultimo_envio = time.time()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error de transcripci√≥n: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# üîÅ INICIAR STREAM Y HILO\n",
    "# -----------------------------\n",
    "stream = sd.InputStream(samplerate=FS, channels=1, dtype='float32',\n",
    "                        callback=audio_callback, blocksize=512,\n",
    "                        device=dispositivo_idx)\n",
    "stream.start()\n",
    "\n",
    "t = threading.Thread(target=transcribir_en_vivo, daemon=True)\n",
    "t.start()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nüõë Transcripci√≥n detenida.\")\n",
    "    stream.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ab823",
   "metadata": {},
   "source": [
    "Y aqu√≠ ahora probamos a meter un modelo clasificador de texto de scam (spam-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba100926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, queue, threading, numpy as np, sounddevice as sd\n",
    "from transformers import pipeline\n",
    "\n",
    "# üîß Configuraci√≥n\n",
    "DEVICE_NAME = \"CABLE Output (VB-Audio Virtual Cable)\"\n",
    "FS = 16000\n",
    "CHUNK_DURATION = 0.8\n",
    "BLOCKSIZE = int(FS * CHUNK_DURATION)\n",
    "ENERGY_THRESHOLD_DB = -40\n",
    "\n",
    "# Pipelines\n",
    "asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"jonatasgrosman/wav2vec2-large-xlsr-53-spanish\",\n",
    "    # device=0  # usa GPU si est√° disponible\n",
    ")\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"skandavivek2/spam-classifier\"\n",
    ")\n",
    "\n",
    "# Funciones auxiliares\n",
    "def buscar_dispositivo(nombre):\n",
    "    for i, dev in enumerate(sd.query_devices()):\n",
    "        if nombre.lower() in dev[\"name\"].lower():\n",
    "            return i\n",
    "    raise ValueError(f\"Dispositivo '{nombre}' no encontrado\")\n",
    "def energia_rms_db(audio):\n",
    "    rms = np.sqrt(np.mean(audio**2))\n",
    "    return -100 if rms == 0 else 20 * np.log10(rms)\n",
    "\n",
    "# Inicializaci√≥n\n",
    "dispositivo_idx = buscar_dispositivo(DEVICE_NAME)\n",
    "audio_q = queue.Queue()\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    if status:\n",
    "        print(\"‚ö†Ô∏è\", status)\n",
    "    audio_q.put(indata.copy())\n",
    "\n",
    "def transcribir_y_clasificar():\n",
    "    buffer = np.empty((0, 1), dtype=np.float32)\n",
    "    texto_acum = \"\"\n",
    "    ult_envio = time.time()\n",
    "\n",
    "    while True:\n",
    "        data = audio_q.get()\n",
    "        buffer = np.concatenate((buffer, data), axis=0)\n",
    "\n",
    "        if len(buffer) >= BLOCKSIZE:\n",
    "            chunk = buffer[:BLOCKSIZE]\n",
    "            buffer = buffer[BLOCKSIZE:]\n",
    "            audio_arr = chunk.squeeze()\n",
    "            if energia_rms_db(audio_arr) < ENERGY_THRESHOLD_DB:\n",
    "                continue\n",
    "\n",
    "            result = asr(audio_arr)  # sin sampling_rate\n",
    "            texto = result[\"text\"].strip()\n",
    "            if len(texto.split()) >= 3:\n",
    "                texto_acum += \" \" + texto\n",
    "                print(\"üó£Ô∏è\", texto)\n",
    "\n",
    "            # Clasificar si hay suficiente texto o tiempo\n",
    "            if len(texto_acum.split()) >= 15 or (time.time() - ult_envio) > 8:\n",
    "                texto_final = texto_acum.strip()\n",
    "                print(\"\\nüìã Texto final:\", texto_final)\n",
    "                cls = classifier(texto_final)[0]\n",
    "                label = cls[\"label\"]\n",
    "                score = cls[\"score\"]\n",
    "                print(f\"‚ö†Ô∏è Clasificaci√≥n: {label} (confianza {score:.2f})\\n\")\n",
    "                texto_acum = \"\"\n",
    "                ult_envio = time.time()\n",
    "\n",
    "# Ejecutar stream y hilo\n",
    "stream = sd.InputStream(samplerate=FS, channels=1, dtype='float32',\n",
    "                        callback=audio_callback, blocksize=512,\n",
    "                        device=dispositivo_idx)\n",
    "stream.start()\n",
    "\n",
    "t = threading.Thread(target=transcribir_y_clasificar, daemon=True)\n",
    "t.start()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    stream.stop()\n",
    "    print(\"üõë Proceso detenido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a16565e",
   "metadata": {},
   "source": [
    "Las transcripciones se ve√≠an mal antes, porque se cortaban, una soluci√≥n a esto es faster_whisper, as√≠ que vamos a probar a meterlo al modelo de clasificaci√≥n, que no s√© si quiera si se trata de un modelo en espa√±ol, pero es simplemente por probar. Seguramente tengamos que hacer un modelo nosotros, porque he buscado pero no he encontrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577754da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, queue, threading, numpy as np, sounddevice as sd\n",
    "from faster_whisper import WhisperModel\n",
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "# üîß Configuraci√≥n\n",
    "DEVICE_NAME = \"CABLE Output (VB-Audio Virtual Cable)\"\n",
    "FS = 16000\n",
    "CHUNK_DURATION = 2.0  # 2‚ÄØs ofrece buen equilibrio fluidez/latencia\n",
    "BLOCKSIZE = int(FS * CHUNK_DURATION)\n",
    "ENERGY_THRESHOLD_DB = -40\n",
    "\n",
    "# Funciones auxiliares\n",
    "def buscar_dispositivo(nombre):\n",
    "    for i, dev in enumerate(sd.query_devices()):\n",
    "        if nombre.lower() in dev[\"name\"].lower():\n",
    "            return i\n",
    "    raise ValueError(f\"Dispositivo '{nombre}' no encontrado\")\n",
    "\n",
    "def energia_rms_db(audio):\n",
    "    rms = np.sqrt(np.mean(audio**2))\n",
    "    return -100 if rms == 0 else 20 * np.log10(rms)\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r'\\bna\\b', 'una', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    return texto.strip()\n",
    "\n",
    "# Carga de modelos\n",
    "print(\"üîÑ Cargando Whisper (faster-whisper)...\")\n",
    "whisper_model = WhisperModel(\"base\", compute_type=\"int8\", device=\"cpu\")\n",
    "  # o \"float16\" si GPU\n",
    "\n",
    "print(\"üîÑ Cargando clasificador de spam...\")\n",
    "classifier = pipeline(\"text-classification\", model=\"skandavivek2/spam-classifier\")\n",
    "\n",
    "# Inicializaci√≥n del dispositivo\n",
    "dispositivo_idx = buscar_dispositivo(DEVICE_NAME)\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    if status:\n",
    "        print(\"‚ö†Ô∏è\", status)\n",
    "    audio_queue.put(indata.copy())\n",
    "\n",
    "# Hilo: transcripci√≥n y clasificaci√≥n\n",
    "def transcribir_y_clasificar():\n",
    "    buffer = np.empty((0, 1), dtype=np.float32)\n",
    "    texto_acum = \"\"\n",
    "    ult_envio = time.time()\n",
    "\n",
    "    print(\"üß† Iniciando transcripci√≥n en vivo con buffer...\\n\")\n",
    "    while True:\n",
    "        data = audio_queue.get()\n",
    "        buffer = np.concatenate((buffer, data), axis=0)\n",
    "\n",
    "        if len(buffer) >= BLOCKSIZE:\n",
    "            chunk = buffer[:BLOCKSIZE]\n",
    "            buffer = buffer[BLOCKSIZE:]\n",
    "            audio_arr = chunk.squeeze()\n",
    "            if energia_rms_db(audio_arr) < ENERGY_THRESHOLD_DB:\n",
    "                continue\n",
    "\n",
    "            # Transcribir con faster-whisper\n",
    "            segments, _ = whisper_model.transcribe(audio_arr, language=\"es\")\n",
    "            texto_fragmento = \" \".join(seg.text for seg in segments).strip()\n",
    "\n",
    "            if not texto_fragmento:\n",
    "                continue\n",
    "\n",
    "            texto_fragmento = limpiar_texto(texto_fragmento)\n",
    "            if len(texto_fragmento.split()) >= 3:\n",
    "                texto_acum += \" \" + texto_fragmento\n",
    "                print(\"üó£Ô∏è\", texto_fragmento)\n",
    "\n",
    "            # Clasificar si hay suficiente contexto o tiempo\n",
    "            if len(texto_acum.split()) >= 20 or (time.time() - ult_envio) > 10:\n",
    "                texto_final = texto_acum.strip()\n",
    "                print(f\"\\nüìã Texto para clasificar:\\n{texto_final}\\n\")\n",
    "                cls = classifier(texto_final)[0]\n",
    "                label = cls[\"label\"]\n",
    "                score = cls[\"score\"]\n",
    "                result_label = \"SPAM\" if label.lower().startswith(\"label_\") == False and label.lower()==\"spam\" else (\"SPAM\" if label.upper()==\"LABEL_1\" else \"NO SPAM\")\n",
    "                print(f\"‚ö†Ô∏è Clasificaci√≥n: **{result_label}** (confianza {score:.2f})\\n\")\n",
    "                texto_acum = \"\"\n",
    "                ult_envio = time.time()\n",
    "\n",
    "# Iniciar audio stream y hilo\n",
    "stream = sd.InputStream(samplerate=FS, channels=1, dtype='float32',\n",
    "                        callback=audio_callback, blocksize=512,\n",
    "                        device=dispositivo_idx)\n",
    "stream.start()\n",
    "thread = threading.Thread(target=transcribir_y_clasificar, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    stream.stop()\n",
    "    print(\"üõë Proceso detenido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d1656",
   "metadata": {},
   "source": [
    "\n",
    "Otra PRUEBA con otro modelo para ver si detecta voz falsa (pista: no lo hace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "# --- 1. Configuraci√≥n ---\n",
    "DEVICE_NAME = \"CABLE Output (VB-Audio Virtual Cable)\"\n",
    "MODELO_DETECCION = \"microsoft/wavlm-base-plus-sd\"\n",
    "SAMPLE_RATE = 16000\n",
    "CHUNK_DURATION = 2.0\n",
    "BLOCKSIZE = int(SAMPLE_RATE * CHUNK_DURATION)\n",
    "ENERGY_THRESHOLD_DB = -40\n",
    "\n",
    "# --- 2. Funciones Auxiliares ---\n",
    "def buscar_dispositivo(nombre):\n",
    "    print(\"Buscando dispositivos de audio...\")\n",
    "    for i, dev in enumerate(sd.query_devices()):\n",
    "        if nombre.lower() in dev[\"name\"].lower():\n",
    "            print(f\"‚úÖ Dispositivo encontrado: {dev['name']}\")\n",
    "            return i\n",
    "    raise ValueError(f\"Dispositivo '{nombre}' no encontrado.\")\n",
    "\n",
    "def energia_rms_db(audio):\n",
    "    rms = np.sqrt(np.mean(audio**2))\n",
    "    return -100 if rms == 0 else 20 * np.log10(rms)\n",
    "\n",
    "# --- 3. Carga del Modelo ---\n",
    "print(f\"üîÑ Cargando el modelo de detecci√≥n '{MODELO_DETECCION}'...\")\n",
    "try:\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(MODELO_DETECCION)\n",
    "    model = AutoModelForAudioClassification.from_pretrained(\n",
    "        MODELO_DETECCION,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    detector_voz = pipeline(\n",
    "        \"audio-classification\", \n",
    "        model=model, \n",
    "        feature_extractor=feature_extractor\n",
    "    )\n",
    "    print(\"‚úÖ Modelo cargado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar el modelo: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. L√≥gica de Captura y Procesamiento (Arquitectura Corregida) ---\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    \"\"\"Esta funci√≥n se llama desde un hilo de alta prioridad por cada bloque de audio.\"\"\"\n",
    "    if status:\n",
    "        print(\"‚ö†Ô∏è\", status)\n",
    "    audio_queue.put(indata.copy())\n",
    "\n",
    "def detectar_voz_ia_thread():\n",
    "    \"\"\"Este hilo saca audio de la cola, lo acumula y lo procesa.\"\"\"\n",
    "    buffer = np.empty((0, 1), dtype=np.float32)\n",
    "    print(\"\\nüß† Iniciando detecci√≥n en vivo con buffer... (Presiona Stop en Jupyter para detener)\")\n",
    "    \n",
    "    while True:\n",
    "        # Saca un micro-bloque de la cola\n",
    "        data = audio_queue.get()\n",
    "        # Lo a√±ade al buffer\n",
    "        buffer = np.concatenate((buffer, data), axis=0)\n",
    "\n",
    "        # Si el buffer ya tiene suficiente audio, lo procesa\n",
    "        if len(buffer) >= BLOCKSIZE:\n",
    "            chunk_a_procesar = buffer[:BLOCKSIZE]\n",
    "            buffer = buffer[BLOCKSIZE:]  # Deja el resto para la siguiente vuelta\n",
    "            \n",
    "            audio_chunk = chunk_a_procesar.squeeze()\n",
    "\n",
    "            if energia_rms_db(audio_chunk) > ENERGY_THRESHOLD_DB:\n",
    "                print(\"\\nüé§ ¬°Voz detectada! Analizando...\", end=\"\", flush=True)\n",
    "\n",
    "                resultado = detector_voz(\n",
    "                    {\"sampling_rate\": SAMPLE_RATE, \"raw\": audio_chunk},\n",
    "                    top_k=2\n",
    "                )\n",
    "                \n",
    "                mejor_resultado = max(resultado, key=lambda x: x['score'])\n",
    "                etiqueta = mejor_resultado['label']\n",
    "                confianza = mejor_resultado['score']\n",
    "                \n",
    "                if etiqueta.lower() == 'bonafide':\n",
    "                    print(f\" - Resultado: Humano (Confianza: {confianza:.2f})\")\n",
    "                elif etiqueta.lower() == 'spoof':\n",
    "                    print(f\" - Resultado: IA Generada (Confianza: {confianza:.2f})\")\n",
    "                else:\n",
    "                    print(f\" - Resultado: {etiqueta} (Confianza: {confianza:.2f})\")\n",
    "\n",
    "# --- 5. Iniciar el Proceso ---\n",
    "try:\n",
    "    dispositivo_idx = buscar_dispositivo(DEVICE_NAME)\n",
    "\n",
    "    # Creamos el stream con el callback y un tama√±o de bloque peque√±o\n",
    "    stream = sd.InputStream(\n",
    "        device=dispositivo_idx,\n",
    "        samplerate=SAMPLE_RATE,\n",
    "        channels=1,\n",
    "        dtype='float32',\n",
    "        blocksize=512,  # Tama√±o de bloque peque√±o, como en tu script\n",
    "        callback=audio_callback\n",
    "    )\n",
    "    stream.start()\n",
    "\n",
    "    # Iniciamos el hilo que procesar√° el audio\n",
    "    thread = threading.Thread(target=detectar_voz_ia_thread, daemon=True)\n",
    "    thread.start()\n",
    "\n",
    "    # El hilo principal puede esperar aqu√≠ o hacer otras cosas\n",
    "    # En Jupyter, la celda se quedar√° \"corriendo\"\n",
    "    while thread.is_alive():\n",
    "        thread.join(0.1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nüõë Proceso detenido.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Ha ocurrido un error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b269f",
   "metadata": {},
   "source": [
    "Probamos pues m√°s cosas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "import torch\n",
    "import librosa # Es buena pr√°ctica tenerlo para el manejo de audio\n",
    "\n",
    "# --- 1. Configuraci√≥n ---\n",
    "# El nuevo modelo que quieres probar.\n",
    "MODELO_DETECCION = \"MattyB95/AST-ASVspoof5-Synthetic-Voice-Detection\"\n",
    "\n",
    "# üñçÔ∏è ¬°IMPORTANTE! Cambia esta l√≠nea para que apunte a tu archivo de audio .wav\n",
    "RUTA_AUDIO = r\"C:\\Users\\pilar\\OneDrive\\Escritorio\\MasterCopia\\TFMPersonal\\fake_audios\\fake_048.wav\"\n",
    "\n",
    "# --- 2. Carga del Modelo ---\n",
    "print(f\"üîÑ Cargando el modelo de detecci√≥n '{MODELO_DETECCION}'...\")\n",
    "try:\n",
    "    # Usamos las l√≠neas que proporcionaste para cargar los componentes\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(MODELO_DETECCION)\n",
    "    model = AutoModelForAudioClassification.from_pretrained(MODELO_DETECCION)\n",
    "\n",
    "    # Creamos el pipeline usando los componentes que ya cargamos\n",
    "    detector_voz = pipeline(\n",
    "        \"audio-classification\",\n",
    "        model=model,\n",
    "        feature_extractor=feature_extractor\n",
    "    )\n",
    "    print(\"‚úÖ Modelo cargado correctamente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar el modelo: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. An√°lisis del Archivo ---\n",
    "print(f\"\\nüéß Analizando el archivo: {RUTA_AUDIO}...\")\n",
    "try:\n",
    "    # Aseguramos que el audio se cargue a la frecuencia de muestreo que el modelo espera\n",
    "    audio_input, sample_rate = librosa.load(RUTA_AUDIO, sr=16000)\n",
    "    \n",
    "    resultado = detector_voz(audio_input, top_k=2)\n",
    "    mejor_resultado = max(resultado, key=lambda x: x['score'])\n",
    "    \n",
    "    etiqueta = mejor_resultado['label']\n",
    "    confianza = mejor_resultado['score']\n",
    "\n",
    "    print(\"\\n--- Resultado del An√°lisis ---\")\n",
    "    # Adaptamos la l√≥gica a las etiquetas de este modelo ('bonafide' y 'spoof')\n",
    "    if etiqueta.lower() == 'bonafide':\n",
    "        print(f\"üó£Ô∏è  La voz es **HUMANA** (Confianza: {confianza:.2f})\")\n",
    "    elif etiqueta.lower() == 'spoof':\n",
    "        print(f\"ü§ñ  La voz es **GENERADA POR IA** (Confianza: {confianza:.2f})\")\n",
    "    else:\n",
    "        print(f\"‚ùì  An√°lisis: {etiqueta} (Confianza: {confianza:.2f})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Ha ocurrido un error al procesar el archivo: {e}\")\n",
    "    print(\"Verifica que la ruta del archivo es correcta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a653e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd57dc8a",
   "metadata": {},
   "source": [
    "# VISUALIZAR SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072dd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "filename_fake = \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/output.wav\"\n",
    "filename_real_sample_4 = \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_4.wav\"\n",
    "filename_real_sample_1 = \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_1.wav\"\n",
    "filename_real_sample_2 = \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_2.wav\"\n",
    "filename_real_sample_3 = \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_3.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "# Cargar se√±al falsa\n",
    "audio_signal, sampling_rate = librosa.load(filename_fake, sr=None)\n",
    "\n",
    "\n",
    "# Cargar se√±al real\n",
    "audio_signal_real_1, sampling_rate_real_1 = librosa.load(filename_real_sample_1, sr=None)\n",
    "audio_signal_real_2, sampling_rate_real_2 = librosa.load(filename_real_sample_2, sr=None)\n",
    "audio_signal_real_3, sampling_rate_real_3 = librosa.load(filename_real_sample_3, sr=None)\n",
    "audio_signal_real_4, sampling_rate_real_4 = librosa.load(filename_real_sample_4, sr=None)\n",
    "# Visualizar las se√±ales\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(5, 1, figsize=(12, 10), sharex=True)\n",
    "librosa.display.waveshow(audio_signal, sr=sampling_rate, ax=axs[0])\n",
    "axs[0].set_title(\"Fake (output.wav)\")\n",
    "librosa.display.waveshow(audio_signal_real_1, sr=sampling_rate_real_1, ax=axs[1])\n",
    "axs[1].set_title(\"Real (sample_1.wav)\")\n",
    "librosa.display.waveshow(audio_signal_real_2, sr=sampling_rate_real_2, ax=axs[2])\n",
    "axs[2].set_title(\"Real (sample_2.wav)\")\n",
    "librosa.display.waveshow(audio_signal_real_3, sr=sampling_rate_real_3, ax=axs[3])\n",
    "axs[3].set_title(\"Real (sample_3.wav)\")\n",
    "librosa.display.waveshow(audio_signal_real_4, sr=sampling_rate_real_4, ax=axs[4])\n",
    "axs[4].set_title(\"Real (sample_4.wav)\")\n",
    "axs[4].set_xlabel(\"Tiempo (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
