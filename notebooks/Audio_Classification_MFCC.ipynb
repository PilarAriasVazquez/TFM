{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd22246",
   "metadata": {},
   "source": [
    "# Generar voz con IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8193d",
   "metadata": {},
   "source": [
    "## INGLÉS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc7991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script para grabar voz \n",
    "import tortoise.api as api\n",
    "import tortoise.utils as utils\n",
    "import os\n",
    "from scipy.io.wavfile import write\n",
    "# Ruta a los clips de voz\n",
    "clips_paths = [\n",
    "    \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_1.wav\",\n",
    "    \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_2.wav\",\n",
    "    \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_3.wav\",\n",
    "    \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_4.wav\"\n",
    "]\n",
    "\n",
    "reference_clips = [utils.audio.load_audio(p, 22050) for p in clips_paths]\n",
    "tts = api.TextToSpeech()\n",
    "pcm_audio = tts.tts_with_preset(\"Hola, soy Pilar Arias y tengo 23 años. Me gusta el café y el agua\", voice_samples=reference_clips, preset='fast')\n",
    "# Guardar el audio generado\n",
    "output_path = \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/output.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e09665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = \"voz_clonada.wav\"\n",
    "write(output_path, 24000, pcm_audio.squeeze().cpu().numpy())\n",
    "print(f\"✅ Audio guardado como {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f429c",
   "metadata": {},
   "source": [
    "## ESPAÑOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6217cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import random\n",
    "from TTS.api import TTS\n",
    "from torchaudio.transforms import MelSpectrogram, MFCC\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.serialization\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "\n",
    "# Añadir XttsConfig como clase segura\n",
    "torch.serialization.add_safe_globals({\"TTS.tts.configs.xtts_config.XttsConfig\": XttsConfig})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99fcf64",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/68835622-2248-8001-9e16-a1eb0cebf76f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b78446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# 1. Generar audios falsos con Coqui XTTS\n",
    "# ============================\n",
    "\n",
    "\n",
    "audio_dir = Path(\"Audios_48samplerate\")\n",
    "speaker_wavs = list(audio_dir.rglob(\"*.wav\"))\n",
    "\n",
    "# todos los audios dentro de Audios_48samplerate\n",
    "\n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n",
    "\n",
    "texts_fake = [\n",
    "    \"Hola, soy del banco, Necesitamos verificar una transacción sospechosa.\",\n",
    "    \"Detectamos un movimiento extraño en tu cuenta, Por favor, confirma tu identidad.\",\n",
    "    \"Se ha intentado acceder a tu cuenta desde otro dispositivo, Activa tu token.\",\n",
    "    \"Necesitamos tu número de tarjeta para validar el pago.\",\n",
    "    \"Buenas tardes, hemos detectado un fallo de seguridad en tu sistema.\",\n",
    "    \"Buenas, le llamamos del banco por un posible fraude en su cuenta.\",\n",
    "    \"Para evitar el bloqueo de su tarjeta, necesitamos el código CVV.\",\n",
    "    \"Su cuenta ha sido comprometida, Necesitamos verificar sus credenciales.\",\n",
    "    \"Hola, soy del servicio de verificación, Necesitamos validar sus datos personales.\",\n",
    "    \"Hemos detectado una compra sospechosa en su cuenta de Amazon.\",\n",
    "    \"Debe validar su identidad para evitar el cierre de su cuenta.\",\n",
    "    \"Llamamos del servicio técnico, Su ordenador está infectado.\",\n",
    "    \"Necesitamos acceso remoto para reparar un fallo en su sistema.\",\n",
    "    \"Tiene un reembolso pendiente, Facilite su número de cuenta.\",\n",
    "    \"Para continuar usando su app, confirme los datos por SMS.\",\n",
    "    \"La policía está investigando su cuenta, Necesitamos colaboración urgente.\",\n",
    "    \"Se ha bloqueado su acceso online por motivos de seguridad.\",\n",
    "    \"Su dispositivo está en riesgo, Debe instalar una herramienta oficial.\",\n",
    "    \"Para evitar multas, actualice su información fiscal ahora.\",\n",
    "    \"Ha ganado un premio, Envíe sus datos para recibirlo.\",\n",
    "    \"Le llamamos de la Seguridad Social, Faltan datos en su expediente.\",\n",
    "    \"Confirmamos que su tarjeta ha sido clonada, Necesitamos los datos actuales.\",\n",
    "    \"Hola, ha recibido una sanción, Evítela accediendo al siguiente enlace.\",\n",
    "    \"Detectamos actividad irregular, Necesitamos comprobar el último movimiento.\",\n",
    "    \"Hay un cargo sospechoso, Para anularlo, indique su número de cuenta.\"\n",
    "] + [\n",
    "    \"Hola, ¿vas a venir hoy a clase?\",\n",
    "    \"Estoy terminando el informe. Te lo paso esta tarde.\",\n",
    "    \"Acuérdate de comprar leche.\",\n",
    "    \"Voy a salir a correr un rato.\",\n",
    "    \"Nos vemos en el cine a las 7.\",\n",
    "    \"¿Te apetece tomar algo esta tarde?\",\n",
    "    \"Voy a hacer la compra, ¿necesitas algo?\",\n",
    "    \"He terminado el informe, te lo envío en un rato.\",\n",
    "    \"¿Te viene bien si pasamos por tu casa a las seis?\",\n",
    "    \"No te olvides de llevar el paraguas.\",\n",
    "    \"Mañana tengo reunión temprano, no podré ir al gimnasio.\",\n",
    "    \"Estamos organizando una cena el viernes. ¿Te apuntas?\",\n",
    "    \"¿Puedes recoger el paquete de la oficina de correos?\",\n",
    "    \"He dejado las llaves en la mesa del salón.\",\n",
    "    \"El examen de mañana empieza a las 9 en punto.\",\n",
    "    \"Voy a preparar algo de cenar, ¿quieres que te guarde?\",\n",
    "    \"No me esperes despierto, llegaré tarde.\",\n",
    "    \"¿Quieres que te mande las fotos del viaje?\",\n",
    "    \"Recuerda que tenemos cita con el médico el jueves.\",\n",
    "    \"Te escribo cuando llegue al trabajo.\",\n",
    "    \"¿Vienes a la reunión o te conectas online?\",\n",
    "    \"Acordamos vernos en la estación a las cinco.\",\n",
    "    \"Lleva abrigo, está haciendo mucho frío.\",\n",
    "    \"Salgo en diez minutos, te aviso al llegar.\",\n",
    "    \"¿Te llamo cuando esté libre o prefieres que te escriba?\"\n",
    "]\n",
    "\n",
    "output_dir = Path(\"fake_audios\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for i, text in enumerate(texts_fake):\n",
    "    wav = tts.tts(text, speaker_wav=speaker_wavs, language=\"es\")\n",
    "    torchaudio.save(str(output_dir / f\"fake_{i:03}.wav\"), torch.tensor([wav]), 24000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e474af",
   "metadata": {},
   "source": [
    "# CLASIFICADOR VOCES MFCC + PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ffb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 2. Extraer MFCCs de audios reales y falsos\n",
    "# =============================================\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "def extract_mfcc(file_path, n_mfcc=13):\n",
    "    y, sr = librosa.load(file_path, sr=24000)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return mfcc.mean(axis=1)\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Audios reales\n",
    "real_dir = \"Audios_48samplerate/Normal\"\n",
    "fake_dir = \"fake_audios\"\n",
    "for file in os.listdir(real_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        features = extract_mfcc(os.path.join(real_dir, file))\n",
    "        data.append(features)\n",
    "        labels.append(0)  # 0 = real\n",
    "\n",
    "# Audios falsos\n",
    "for file in os.listdir(fake_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        features = extract_mfcc(os.path.join(\"fake_audios\", file))\n",
    "        data.append(features)\n",
    "        labels.append(1)  # 1 = fake\n",
    "\n",
    "X = np.array(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "# =============================================\n",
    "# 3. Clasificador con PyTorch\n",
    "# =============================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Convertir a tensores con tipos correctos\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Dividir en train, validación y test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "# 0.6 train / 0.2 val / 0.2 test\n",
    "\n",
    "# Datasets y dataloaders\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds = TensorDataset(X_val, y_val)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=8)\n",
    "test_dl = DataLoader(test_ds, batch_size=8)\n",
    "\n",
    "# Modelo\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(13, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Entrenamiento\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Classifier().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(40):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            acc += (pred.argmax(1) == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    print(f\"📊 Epoch {epoch+1:02d} - Validación: Accuracy = {acc/total:.2%}\")\n",
    "\n",
    "# Evaluación en test\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        pred = model(xb).argmax(1).cpu()\n",
    "        all_preds.extend(pred.numpy())\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "print(\"\\n🧪 Resultados en Test:\")\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    labels=[0, 1],\n",
    "    target_names=[\"Real\", \"Falsa\"],\n",
    "    zero_division=0  # evita errores si una clase no está presente\n",
    "))\n",
    "\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ec529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Distribución original:\", Counter(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3184b4",
   "metadata": {},
   "source": [
    "# CLASIFICADOR VOCES PRE-ENTRENADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cae86",
   "metadata": {},
   "source": [
    "## Detecta todo como fake :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e9f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "# Carga del modelo y extractor\n",
    "\n",
    "##################################33 TODO FAKE\n",
    "# model = AutoModelForAudioClassification.from_pretrained(\"mo-thecreator/Deepfake-audio-detection\")\n",
    "# feature_extractor = AutoFeatureExtractor.from_pretrained(\"mo-thecreator/Deepfake-audio-detection\")\n",
    "\n",
    "\n",
    "##################################33 TODO FAKE\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"langulor/deepfake-voice-spanish\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"langulor/deepfake-voice-spanish\")\n",
    "\n",
    "\n",
    "\n",
    "# Cargar un archivo de audio (.wav, 16kHz)\n",
    "waveform, sample_rate = torchaudio.load(\"Audios_48samplerate/Normal/sample_1_20250721_173507.wav\")\n",
    "waveform = waveform.mean(dim=0)  # Convertir a mono si está en estéreo\n",
    "\n",
    "# Resamplear si es necesario\n",
    "if sample_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "    sample_rate = 16000\n",
    "\n",
    "# Extraer características\n",
    "inputs = feature_extractor(waveform.numpy(), sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "\n",
    "classes = model.config.id2label\n",
    "print(f\"Clases: {classes}\")\n",
    "# Clasificación\n",
    "with torch.no_grad(): #evaluar el modelo sin calcular gradientes\n",
    "    model.eval()  # Poner el modelo en modo evaluación\n",
    "    logits = model(**inputs).logits\n",
    "    probs = softmax(logits, dim=0)\n",
    "    predicted_class = torch.argmax(logits).item()\n",
    "\n",
    "# Interpretar resultado\n",
    "if predicted_class == 0:\n",
    "    print(\"🔊 El audio parece **real**.\")\n",
    "else:\n",
    "    print(\"⚠️ El audio parece **deepfake**.\")\n",
    "\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos audio output.wav\n",
    "waveform, sample_rate = torchaudio.load(\"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/output.wav\")\n",
    "waveform = waveform.mean(dim=0)  # Convertir a mono si está en estéreo\n",
    "print(sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4e40c",
   "metadata": {},
   "source": [
    "# AUDIO TO TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f36d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingsound import SpeechRecognitionModel\n",
    "import os\n",
    "from pathlib import Path\n",
    "model = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-spanish\")\n",
    "# audio_paths = [r\"fake_audios\\fake_000.wav\", r\"Audios_48samplerate\\Normal\\sample_4_20250721_173627.wav\"]\n",
    "\n",
    "# all audios in Audios_48samplerate/Normal\n",
    "audio_paths = [str(p) for p in Path(\"Audios_48samplerate/Normal\").rglob(\"*.wav\")]\n",
    "\n",
    "transcriptions = model.transcribe(audio_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcbfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get transcriptions\n",
    "for i, transcription in enumerate(transcriptions):\n",
    "    print(f\"Transcripción {i+1}:    {transcription['transcription']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a953f",
   "metadata": {},
   "source": [
    "## La cosa ahora sería integrar esto en un audio que se está generando en vivo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee5ff9f",
   "metadata": {},
   "source": [
    "La siguiente celda de código cuenta con: \n",
    "\n",
    "- CHUNK_DURATION = 0.8 // latencia muy baja (~1 s) -> nos da el texto asap con el menor delay\n",
    "\n",
    "- blocksize pequeño (blocksize=512)  el callback se activa rápido.\n",
    "\n",
    "- Umbral de energía RMS (threshold_db = -40) para detectar si hay voz antes de transcribir // así ahorramos coste computacional\n",
    "\n",
    "- Todo en memoria, sin escribir WAVs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bec13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from transformers import pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# 🔧 CONFIGURACIÓN DEL AUDIO\n",
    "# -----------------------------\n",
    "DEVICE_NAME = \"CABLE Output (VB-Audio Virtual Cable)\"\n",
    "FS = 16000              # Wav2Vec2 requiere 16kHz\n",
    "CHUNK_DURATION = 0.8    # Segundos por fragmento (baja latencia)\n",
    "BLOCKSIZE = int(FS * CHUNK_DURATION)\n",
    "ENERGY_THRESHOLD_DB = -40  # Umbral en decibelios para detección de voz\n",
    "\n",
    "# Buscar índice del dispositivo\n",
    "def buscar_dispositivo(nombre_dispositivo):\n",
    "    for i, dev in enumerate(sd.query_devices()):\n",
    "        if nombre_dispositivo.lower() in dev['name'].lower():\n",
    "            return i\n",
    "    raise ValueError(f\"Dispositivo '{nombre_dispositivo}' no encontrado.\")\n",
    "\n",
    "try:\n",
    "    dispositivo_idx = buscar_dispositivo(DEVICE_NAME)\n",
    "    print(f\"🎧 Usando dispositivo: {DEVICE_NAME} (índice {dispositivo_idx})\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# -----------------------------\n",
    "# 🧠 CARGAR MODELO wav2vec2\n",
    "# -----------------------------\n",
    "print(\"🔄 Cargando modelo...\")\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"jonatasgrosman/wav2vec2-large-xlsr-53-spanish\",\n",
    "    device=0 if sd.query_devices(kind='input')['hostapi'] == 1 else -1  # GPU si disponible\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 🎙️ CAPTURA Y TRANSCRIPCIÓN\n",
    "# -----------------------------\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    if status:\n",
    "        print(f\"⚠️ Estado del audio: {status}\")\n",
    "    audio_queue.put(indata.copy())\n",
    "\n",
    "def energia_rms_db(audio_array):\n",
    "    # RMS -> log en dB\n",
    "    rms = np.sqrt(np.mean(audio_array**2))\n",
    "    if rms == 0:\n",
    "        return -100\n",
    "    return 20 * np.log10(rms)\n",
    "\n",
    "def transcribir_en_vivo():\n",
    "    print(\"🧠 Transcribiendo en tiempo real con buffer de texto...\\n\")\n",
    "    buffer = np.empty((0, 1), dtype=np.float32)\n",
    "    texto_acumulado = \"\"\n",
    "    tiempo_ultimo_envio = time.time()\n",
    "\n",
    "    while True:\n",
    "        data = audio_queue.get()\n",
    "        buffer = np.concatenate((buffer, data), axis=0)\n",
    "\n",
    "        if len(buffer) >= BLOCKSIZE:\n",
    "            chunk = buffer[:BLOCKSIZE]\n",
    "            buffer = buffer[BLOCKSIZE:]\n",
    "\n",
    "            audio_array = chunk.squeeze()\n",
    "            energia_db = energia_rms_db(audio_array)\n",
    "\n",
    "            if energia_db < ENERGY_THRESHOLD_DB:\n",
    "                print(f\"🔇 Fragmento descartado (RMS = {energia_db:.1f} dB)\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                result = asr_pipeline(audio_array)\n",
    "                texto = result['text'].strip()\n",
    "\n",
    "                # Filtro: solo añadir si tiene 3 o más palabras\n",
    "                if len(texto.split()) >= 3:\n",
    "                    texto_acumulado += \" \" + texto\n",
    "                    print(\"🗣️\", texto)\n",
    "\n",
    "                # Enviar al clasificador si ha pasado suficiente tiempo o el buffer es largo\n",
    "                if len(texto_acumulado.split()) >= 20 or (time.time() - tiempo_ultimo_envio) > 10:\n",
    "                    if texto_acumulado:\n",
    "                        print(\"\\n🔎 Texto para clasificación:\", texto_acumulado.strip())\n",
    "\n",
    "                        texto_acumulado = \"\"\n",
    "                        tiempo_ultimo_envio = time.time()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error de transcripción: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 🔁 INICIAR STREAM Y HILO\n",
    "# -----------------------------\n",
    "stream = sd.InputStream(samplerate=FS, channels=1, dtype='float32',\n",
    "                        callback=audio_callback, blocksize=512,\n",
    "                        device=dispositivo_idx)\n",
    "stream.start()\n",
    "\n",
    "t = threading.Thread(target=transcribir_en_vivo, daemon=True)\n",
    "t.start()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n🛑 Transcripción detenida.\")\n",
    "    stream.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ab823",
   "metadata": {},
   "source": [
    "Y aquí ahora probamos a meter un modelo clasificador de texto de scam (spam-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba100926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, queue, threading, numpy as np, sounddevice as sd\n",
    "from transformers import pipeline\n",
    "\n",
    "# 🔧 Configuración\n",
    "DEVICE_NAME = \"CABLE Output (VB-Audio Virtual Cable)\"\n",
    "FS = 16000\n",
    "CHUNK_DURATION = 0.8\n",
    "BLOCKSIZE = int(FS * CHUNK_DURATION)\n",
    "ENERGY_THRESHOLD_DB = -40\n",
    "\n",
    "# Pipelines\n",
    "asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"jonatasgrosman/wav2vec2-large-xlsr-53-spanish\",\n",
    "    # device=0  # usa GPU si está disponible\n",
    ")\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"skandavivek2/spam-classifier\"\n",
    ")\n",
    "\n",
    "# Funciones auxiliares\n",
    "def buscar_dispositivo(nombre):\n",
    "    for i, dev in enumerate(sd.query_devices()):\n",
    "        if nombre.lower() in dev[\"name\"].lower():\n",
    "            return i\n",
    "    raise ValueError(f\"Dispositivo '{nombre}' no encontrado\")\n",
    "def energia_rms_db(audio):\n",
    "    rms = np.sqrt(np.mean(audio**2))\n",
    "    return -100 if rms == 0 else 20 * np.log10(rms)\n",
    "\n",
    "# Inicialización\n",
    "dispositivo_idx = buscar_dispositivo(DEVICE_NAME)\n",
    "audio_q = queue.Queue()\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    if status:\n",
    "        print(\"⚠️\", status)\n",
    "    audio_q.put(indata.copy())\n",
    "\n",
    "def transcribir_y_clasificar():\n",
    "    buffer = np.empty((0, 1), dtype=np.float32)\n",
    "    texto_acum = \"\"\n",
    "    ult_envio = time.time()\n",
    "\n",
    "    while True:\n",
    "        data = audio_q.get()\n",
    "        buffer = np.concatenate((buffer, data), axis=0)\n",
    "\n",
    "        if len(buffer) >= BLOCKSIZE:\n",
    "            chunk = buffer[:BLOCKSIZE]\n",
    "            buffer = buffer[BLOCKSIZE:]\n",
    "            audio_arr = chunk.squeeze()\n",
    "            if energia_rms_db(audio_arr) < ENERGY_THRESHOLD_DB:\n",
    "                continue\n",
    "\n",
    "            result = asr(audio_arr)  # sin sampling_rate\n",
    "            texto = result[\"text\"].strip()\n",
    "            if len(texto.split()) >= 3:\n",
    "                texto_acum += \" \" + texto\n",
    "                print(\"🗣️\", texto)\n",
    "\n",
    "            # Clasificar si hay suficiente texto o tiempo\n",
    "            if len(texto_acum.split()) >= 15 or (time.time() - ult_envio) > 8:\n",
    "                texto_final = texto_acum.strip()\n",
    "                print(\"\\n📋 Texto final:\", texto_final)\n",
    "                cls = classifier(texto_final)[0]\n",
    "                label = cls[\"label\"]\n",
    "                score = cls[\"score\"]\n",
    "                print(f\"⚠️ Clasificación: {label} (confianza {score:.2f})\\n\")\n",
    "                texto_acum = \"\"\n",
    "                ult_envio = time.time()\n",
    "\n",
    "# Ejecutar stream y hilo\n",
    "stream = sd.InputStream(samplerate=FS, channels=1, dtype='float32',\n",
    "                        callback=audio_callback, blocksize=512,\n",
    "                        device=dispositivo_idx)\n",
    "stream.start()\n",
    "\n",
    "t = threading.Thread(target=transcribir_y_clasificar, daemon=True)\n",
    "t.start()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    stream.stop()\n",
    "    print(\"🛑 Proceso detenido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a16565e",
   "metadata": {},
   "source": [
    "Las transcripciones se veían mal antes, porque se cortaban, una solución a esto es faster_whisper, así que vamos a probar a meterlo al modelo de clasificación, que no sé si quiera si se trata de un modelo en español, pero es simplemente por probar. Seguramente tengamos que hacer un modelo nosotros, porque he buscado pero no he encontrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577754da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, queue, threading, numpy as np, sounddevice as sd\n",
    "from faster_whisper import WhisperModel\n",
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "# 🔧 Configuración\n",
    "DEVICE_NAME = \"CABLE Output (VB-Audio Virtual Cable)\"\n",
    "FS = 16000\n",
    "CHUNK_DURATION = 2.0  # 2 s ofrece buen equilibrio fluidez/latencia\n",
    "BLOCKSIZE = int(FS * CHUNK_DURATION)\n",
    "ENERGY_THRESHOLD_DB = -40\n",
    "\n",
    "# Funciones auxiliares\n",
    "def buscar_dispositivo(nombre):\n",
    "    for i, dev in enumerate(sd.query_devices()):\n",
    "        if nombre.lower() in dev[\"name\"].lower():\n",
    "            return i\n",
    "    raise ValueError(f\"Dispositivo '{nombre}' no encontrado\")\n",
    "\n",
    "def energia_rms_db(audio):\n",
    "    rms = np.sqrt(np.mean(audio**2))\n",
    "    return -100 if rms == 0 else 20 * np.log10(rms)\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r'\\bna\\b', 'una', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    return texto.strip()\n",
    "\n",
    "# Carga de modelos\n",
    "print(\"🔄 Cargando Whisper (faster-whisper)...\")\n",
    "whisper_model = WhisperModel(\"base\", compute_type=\"int8\", device=\"cpu\")\n",
    "  # o \"float16\" si GPU\n",
    "\n",
    "print(\"🔄 Cargando clasificador de spam...\")\n",
    "classifier = pipeline(\"text-classification\", model=\"skandavivek2/spam-classifier\")\n",
    "\n",
    "# Inicialización del dispositivo\n",
    "dispositivo_idx = buscar_dispositivo(DEVICE_NAME)\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    if status:\n",
    "        print(\"⚠️\", status)\n",
    "    audio_queue.put(indata.copy())\n",
    "\n",
    "# Hilo: transcripción y clasificación\n",
    "def transcribir_y_clasificar():\n",
    "    buffer = np.empty((0, 1), dtype=np.float32)\n",
    "    texto_acum = \"\"\n",
    "    ult_envio = time.time()\n",
    "\n",
    "    print(\"🧠 Iniciando transcripción en vivo con buffer...\\n\")\n",
    "    while True:\n",
    "        data = audio_queue.get()\n",
    "        buffer = np.concatenate((buffer, data), axis=0)\n",
    "\n",
    "        if len(buffer) >= BLOCKSIZE:\n",
    "            chunk = buffer[:BLOCKSIZE]\n",
    "            buffer = buffer[BLOCKSIZE:]\n",
    "            audio_arr = chunk.squeeze()\n",
    "            if energia_rms_db(audio_arr) < ENERGY_THRESHOLD_DB:\n",
    "                continue\n",
    "\n",
    "            # Transcribir con faster-whisper\n",
    "            segments, _ = whisper_model.transcribe(audio_arr, language=\"es\")\n",
    "            texto_fragmento = \" \".join(seg.text for seg in segments).strip()\n",
    "\n",
    "            if not texto_fragmento:\n",
    "                continue\n",
    "\n",
    "            texto_fragmento = limpiar_texto(texto_fragmento)\n",
    "            if len(texto_fragmento.split()) >= 3:\n",
    "                texto_acum += \" \" + texto_fragmento\n",
    "                print(\"🗣️\", texto_fragmento)\n",
    "\n",
    "            # Clasificar si hay suficiente contexto o tiempo\n",
    "            if len(texto_acum.split()) >= 20 or (time.time() - ult_envio) > 10:\n",
    "                texto_final = texto_acum.strip()\n",
    "                print(f\"\\n📋 Texto para clasificar:\\n{texto_final}\\n\")\n",
    "                cls = classifier(texto_final)[0]\n",
    "                label = cls[\"label\"]\n",
    "                score = cls[\"score\"]\n",
    "                result_label = \"SPAM\" if label.lower().startswith(\"label_\") == False and label.lower()==\"spam\" else (\"SPAM\" if label.upper()==\"LABEL_1\" else \"NO SPAM\")\n",
    "                print(f\"⚠️ Clasificación: **{result_label}** (confianza {score:.2f})\\n\")\n",
    "                texto_acum = \"\"\n",
    "                ult_envio = time.time()\n",
    "\n",
    "# Iniciar audio stream y hilo\n",
    "stream = sd.InputStream(samplerate=FS, channels=1, dtype='float32',\n",
    "                        callback=audio_callback, blocksize=512,\n",
    "                        device=dispositivo_idx)\n",
    "stream.start()\n",
    "thread = threading.Thread(target=transcribir_y_clasificar, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    stream.stop()\n",
    "    print(\"🛑 Proceso detenido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d1656",
   "metadata": {},
   "source": [
    "\n",
    "Otra PRUEBA con otro modelo para ver si detecta voz falsa (pista: no lo hace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from transformers import pipeline, AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "# --- 1. Configuración ---\n",
    "DEVICE_NAME = \"CABLE Output (VB-Audio Virtual Cable)\"\n",
    "MODELO_DETECCION = \"microsoft/wavlm-base-plus-sd\"\n",
    "SAMPLE_RATE = 16000\n",
    "CHUNK_DURATION = 2.0\n",
    "BLOCKSIZE = int(SAMPLE_RATE * CHUNK_DURATION)\n",
    "ENERGY_THRESHOLD_DB = -40\n",
    "\n",
    "# --- 2. Funciones Auxiliares ---\n",
    "def buscar_dispositivo(nombre):\n",
    "    print(\"Buscando dispositivos de audio...\")\n",
    "    for i, dev in enumerate(sd.query_devices()):\n",
    "        if nombre.lower() in dev[\"name\"].lower():\n",
    "            print(f\"✅ Dispositivo encontrado: {dev['name']}\")\n",
    "            return i\n",
    "    raise ValueError(f\"Dispositivo '{nombre}' no encontrado.\")\n",
    "\n",
    "def energia_rms_db(audio):\n",
    "    rms = np.sqrt(np.mean(audio**2))\n",
    "    return -100 if rms == 0 else 20 * np.log10(rms)\n",
    "\n",
    "# --- 3. Carga del Modelo ---\n",
    "print(f\"🔄 Cargando el modelo de detección '{MODELO_DETECCION}'...\")\n",
    "try:\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(MODELO_DETECCION)\n",
    "    model = AutoModelForAudioClassification.from_pretrained(\n",
    "        MODELO_DETECCION,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    detector_voz = pipeline(\n",
    "        \"audio-classification\", \n",
    "        model=model, \n",
    "        feature_extractor=feature_extractor\n",
    "    )\n",
    "    print(\"✅ Modelo cargado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al cargar el modelo: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. Lógica de Captura y Procesamiento (Arquitectura Corregida) ---\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    \"\"\"Esta función se llama desde un hilo de alta prioridad por cada bloque de audio.\"\"\"\n",
    "    if status:\n",
    "        print(\"⚠️\", status)\n",
    "    audio_queue.put(indata.copy())\n",
    "\n",
    "def detectar_voz_ia_thread():\n",
    "    \"\"\"Este hilo saca audio de la cola, lo acumula y lo procesa.\"\"\"\n",
    "    buffer = np.empty((0, 1), dtype=np.float32)\n",
    "    print(\"\\n🧠 Iniciando detección en vivo con buffer... (Presiona Stop en Jupyter para detener)\")\n",
    "    \n",
    "    while True:\n",
    "        # Saca un micro-bloque de la cola\n",
    "        data = audio_queue.get()\n",
    "        # Lo añade al buffer\n",
    "        buffer = np.concatenate((buffer, data), axis=0)\n",
    "\n",
    "        # Si el buffer ya tiene suficiente audio, lo procesa\n",
    "        if len(buffer) >= BLOCKSIZE:\n",
    "            chunk_a_procesar = buffer[:BLOCKSIZE]\n",
    "            buffer = buffer[BLOCKSIZE:]  # Deja el resto para la siguiente vuelta\n",
    "            \n",
    "            audio_chunk = chunk_a_procesar.squeeze()\n",
    "\n",
    "            if energia_rms_db(audio_chunk) > ENERGY_THRESHOLD_DB:\n",
    "                print(\"\\n🎤 ¡Voz detectada! Analizando...\", end=\"\", flush=True)\n",
    "\n",
    "                resultado = detector_voz(\n",
    "                    {\"sampling_rate\": SAMPLE_RATE, \"raw\": audio_chunk},\n",
    "                    top_k=2\n",
    "                )\n",
    "                \n",
    "                mejor_resultado = max(resultado, key=lambda x: x['score'])\n",
    "                etiqueta = mejor_resultado['label']\n",
    "                confianza = mejor_resultado['score']\n",
    "                \n",
    "                if etiqueta.lower() == 'bonafide':\n",
    "                    print(f\" - Resultado: Humano (Confianza: {confianza:.2f})\")\n",
    "                elif etiqueta.lower() == 'spoof':\n",
    "                    print(f\" - Resultado: IA Generada (Confianza: {confianza:.2f})\")\n",
    "                else:\n",
    "                    print(f\" - Resultado: {etiqueta} (Confianza: {confianza:.2f})\")\n",
    "\n",
    "# --- 5. Iniciar el Proceso ---\n",
    "try:\n",
    "    dispositivo_idx = buscar_dispositivo(DEVICE_NAME)\n",
    "\n",
    "    # Creamos el stream con el callback y un tamaño de bloque pequeño\n",
    "    stream = sd.InputStream(\n",
    "        device=dispositivo_idx,\n",
    "        samplerate=SAMPLE_RATE,\n",
    "        channels=1,\n",
    "        dtype='float32',\n",
    "        blocksize=512,  # Tamaño de bloque pequeño, como en tu script\n",
    "        callback=audio_callback\n",
    "    )\n",
    "    stream.start()\n",
    "\n",
    "    # Iniciamos el hilo que procesará el audio\n",
    "    thread = threading.Thread(target=detectar_voz_ia_thread, daemon=True)\n",
    "    thread.start()\n",
    "\n",
    "    # El hilo principal puede esperar aquí o hacer otras cosas\n",
    "    # En Jupyter, la celda se quedará \"corriendo\"\n",
    "    while thread.is_alive():\n",
    "        thread.join(0.1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n🛑 Proceso detenido.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Ha ocurrido un error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b269f",
   "metadata": {},
   "source": [
    "Probamos pues más cosas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "import torch\n",
    "import librosa # Es buena práctica tenerlo para el manejo de audio\n",
    "\n",
    "# --- 1. Configuración ---\n",
    "# El nuevo modelo que quieres probar.\n",
    "MODELO_DETECCION = \"MattyB95/AST-ASVspoof5-Synthetic-Voice-Detection\"\n",
    "\n",
    "# 🖍️ ¡IMPORTANTE! Cambia esta línea para que apunte a tu archivo de audio .wav\n",
    "RUTA_AUDIO = r\"C:\\Users\\pilar\\OneDrive\\Escritorio\\MasterCopia\\TFMPersonal\\fake_audios\\fake_048.wav\"\n",
    "\n",
    "# --- 2. Carga del Modelo ---\n",
    "print(f\"🔄 Cargando el modelo de detección '{MODELO_DETECCION}'...\")\n",
    "try:\n",
    "    # Usamos las líneas que proporcionaste para cargar los componentes\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(MODELO_DETECCION)\n",
    "    model = AutoModelForAudioClassification.from_pretrained(MODELO_DETECCION)\n",
    "\n",
    "    # Creamos el pipeline usando los componentes que ya cargamos\n",
    "    detector_voz = pipeline(\n",
    "        \"audio-classification\",\n",
    "        model=model,\n",
    "        feature_extractor=feature_extractor\n",
    "    )\n",
    "    print(\"✅ Modelo cargado correctamente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al cargar el modelo: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Análisis del Archivo ---\n",
    "print(f\"\\n🎧 Analizando el archivo: {RUTA_AUDIO}...\")\n",
    "try:\n",
    "    # Aseguramos que el audio se cargue a la frecuencia de muestreo que el modelo espera\n",
    "    audio_input, sample_rate = librosa.load(RUTA_AUDIO, sr=16000)\n",
    "    \n",
    "    resultado = detector_voz(audio_input, top_k=2)\n",
    "    mejor_resultado = max(resultado, key=lambda x: x['score'])\n",
    "    \n",
    "    etiqueta = mejor_resultado['label']\n",
    "    confianza = mejor_resultado['score']\n",
    "\n",
    "    print(\"\\n--- Resultado del Análisis ---\")\n",
    "    # Adaptamos la lógica a las etiquetas de este modelo ('bonafide' y 'spoof')\n",
    "    if etiqueta.lower() == 'bonafide':\n",
    "        print(f\"🗣️  La voz es **HUMANA** (Confianza: {confianza:.2f})\")\n",
    "    elif etiqueta.lower() == 'spoof':\n",
    "        print(f\"🤖  La voz es **GENERADA POR IA** (Confianza: {confianza:.2f})\")\n",
    "    else:\n",
    "        print(f\"❓  Análisis: {etiqueta} (Confianza: {confianza:.2f})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Ha ocurrido un error al procesar el archivo: {e}\")\n",
    "    print(\"Verifica que la ruta del archivo es correcta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a653e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd57dc8a",
   "metadata": {},
   "source": [
    "# VISUALIZAR SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072dd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "filename_fake = \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/output.wav\"\n",
    "filename_real_sample_4 = \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_4.wav\"\n",
    "filename_real_sample_1 = \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_1.wav\"\n",
    "filename_real_sample_2 = \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_2.wav\"\n",
    "filename_real_sample_3 = \"C:/Users/pilar/OneDrive/Escritorio/MasterCopia/TFMPersonal/Audios/sample_3.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "# Cargar señal falsa\n",
    "audio_signal, sampling_rate = librosa.load(filename_fake, sr=None)\n",
    "\n",
    "\n",
    "# Cargar señal real\n",
    "audio_signal_real_1, sampling_rate_real_1 = librosa.load(filename_real_sample_1, sr=None)\n",
    "audio_signal_real_2, sampling_rate_real_2 = librosa.load(filename_real_sample_2, sr=None)\n",
    "audio_signal_real_3, sampling_rate_real_3 = librosa.load(filename_real_sample_3, sr=None)\n",
    "audio_signal_real_4, sampling_rate_real_4 = librosa.load(filename_real_sample_4, sr=None)\n",
    "# Visualizar las señales\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(5, 1, figsize=(12, 10), sharex=True)\n",
    "librosa.display.waveshow(audio_signal, sr=sampling_rate, ax=axs[0])\n",
    "axs[0].set_title(\"Fake (output.wav)\")\n",
    "librosa.display.waveshow(audio_signal_real_1, sr=sampling_rate_real_1, ax=axs[1])\n",
    "axs[1].set_title(\"Real (sample_1.wav)\")\n",
    "librosa.display.waveshow(audio_signal_real_2, sr=sampling_rate_real_2, ax=axs[2])\n",
    "axs[2].set_title(\"Real (sample_2.wav)\")\n",
    "librosa.display.waveshow(audio_signal_real_3, sr=sampling_rate_real_3, ax=axs[3])\n",
    "axs[3].set_title(\"Real (sample_3.wav)\")\n",
    "librosa.display.waveshow(audio_signal_real_4, sr=sampling_rate_real_4, ax=axs[4])\n",
    "axs[4].set_title(\"Real (sample_4.wav)\")\n",
    "axs[4].set_xlabel(\"Tiempo (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
